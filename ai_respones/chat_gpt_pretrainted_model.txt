How do you run a pretrained model like distilgpt2 in python

Running a pretrained model like **DistilGPT-2** in Python is straightforward using the **Hugging Face Transformers** library. I’ll walk you through the steps carefully.

---

### **1. Install dependencies**

You need `transformers` and optionally `torch`:

```bash
pip install transformers torch
```

---

### **2. Load the model and tokenizer**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")

# Load the pretrained model
model = AutoModelForCausalLM.from_pretrained("distilgpt2")
```

---

### **3. Encode input text**

Transform your input text into token IDs that the model can process.

```python
input_text = "Once upon a time"
inputs = tokenizer(input_text, return_tensors="pt")  # pt = PyTorch tensors
```

---

### **4. Generate text**

Use the model to generate text:

```python
# Generate output tokens
outputs = model.generate(
    **inputs,
    max_length=50,  # maximum length of generated text
    do_sample=True, # use sampling
    top_k=50,       # consider top_k tokens at each step
    top_p=0.95      # nucleus sampling
)

# Decode generated tokens back to text
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(generated_text)
```

---

✅ **Notes:**

* `max_length` controls how long the output can be.
* `do_sample=True` allows for randomness (more creative output).
* `top_k` and `top_p` control the sampling strategy to avoid repetitive output.
* For deterministic output, you can use `do_sample=False`.

---

If you want, I can give you a **complete minimal Python script** that runs DistilGPT-2 and prints multiple generations in one go, ready to run.

Do you want me to do that?
